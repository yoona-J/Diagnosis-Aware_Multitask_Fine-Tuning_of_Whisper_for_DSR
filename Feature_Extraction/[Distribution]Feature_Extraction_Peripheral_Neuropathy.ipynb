{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76dc94-f5cb-4593-829b-2937f669befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydub.utils import db_to_float\n",
    "import itertools\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import IPython.display as ipd\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "import noisereduce as nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfd80f4-1d27-474f-9b93-ac97c58827cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub.utils import which\n",
    "print(\"ffmpeg location:\", which(\"ffmpeg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55ff66-0292-4a1e-a402-f374daa77377",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path(\"YOUR_ROOT\")\n",
    "\n",
    "Peripheral_Neuropathy_AUDIO = BASE_PATH / \"Training/audio_data/TS01/Peripheral_Neuropathy\"\n",
    "Peripheral_Neuropathy_LABEL = BASE_PATH / \"Training/label_data/TL01/Peripheral_Neuropathy\"\n",
    "\n",
    "Cerebral_Palsy_AUDIO = BASE_PATH / \"Validation/audio_data/VS01/Cerebral_Palsy_disease\"\n",
    "Cerebral_Palsy_LABEL = BASE_PATH / \"Validation/label_data/VL01/Cerebral_Palsy_disease\"\n",
    "\n",
    "Stroke_AUDIO = BASE_PATH / \"Validation/audio_data/VS01/Stroke\"\n",
    "Stroke_LABEL = BASE_PATH / \"Validation/label_data/VL01/Stroke\"\n",
    "\n",
    "OUTPUT_BASE = BASE_PATH / \"Preprocessed\"\n",
    "Peripheral_Neuropathy_OUTPUT = OUTPUT_BASE / \"Peripheral_Neuropathy_dataset\"\n",
    "Cerebral_Palsy_OUTPUT = OUTPUT_BASE / \"Cerebral_Palsy_dataset\"\n",
    "Stroke_OUTPUT = OUTPUT_BASE / \"Stroke_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e57fe6-82bc-4365-bc8a-b4acc54010bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Peripheral_Neuropathy →\", Peripheral_Neuropathy_OUTPUT)\n",
    "print(\"Cerebral_Palsy →\", Cerebral_Palsy_OUTPUT)\n",
    "print(\"Stroke →\", Stroke_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e2570-5ad0-464b-a195-627af855acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_OUTPUT_BASE = BASE_PATH / \"Text_Preprocessed\"\n",
    "Peripheral_Neuropathy_TEXT_OUTPUT = TEXT_OUTPUT_BASE / \"Peripheral_Neuropathy_dataset\"\n",
    "\n",
    "Peripheral_Neuropathy_TEXT_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Peripheral_Neuropathy →\", Peripheral_Neuropathy_TEXT_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41fd9d-cf7f-47ee-93d3-136066b4eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fluency_OUTPUT_BASE = BASE_PATH / 'Fluency_Metirx'\n",
    "Peripheral_Neuropathy_Fluency = Fluency_OUTPUT_BASE / 'Peripheral_Neuropathy_Fluency'\n",
    "\n",
    "Peripheral_Neuropathy_Fluency.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ffef43-50fd-467e-a46c-330a762ca70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub.utils import mediainfo\n",
    "\n",
    "df = pd.read_csv(\"preprocessing.csv\")\n",
    "\n",
    "df['Merged_File_ids'] = df['Merged_File_ids'].str.split(',')\n",
    "df = df.explode('Merged_File_ids').reset_index(drop=True)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7a2b1-c520-4246-89f8-0715e1e756c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "Fluency_files = [f for f in os.listdir(Peripheral_Neuropathy_Fluency) if f.endswith(\".wav\")]\n",
    "\n",
    "def extract_person_code(filename):\n",
    "    match = re.search(r'output_PN_(.+)_\\d+\\.wav', filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "person_codes_in_output = [extract_person_code(f) for f in Fluency_files]\n",
    "person_codes_in_output = [code for code in person_codes_in_output if code is not None]\n",
    "\n",
    "def count_matches_per_file(row):\n",
    "    fname = row['Merged_File_ids']\n",
    "    match = re.search(r'ID-\\d{2}-\\d{2}-[A-Z]-(.+?)\\.wav', fname)\n",
    "    if match:\n",
    "        person_code = match.group(1)\n",
    "        return person_codes_in_output.count(person_code)\n",
    "    return 0\n",
    "\n",
    "df['File_Count'] = df.apply(count_matches_per_file, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7bcc17-b717-4d35-b1c6-77a6e927cc08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[(df['Disease'] == 2) & (df['File_Count'] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffbd698-f368-4e1b-a829-6f33ada67656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[(df['Disease'] == 2) & (df['File_Count'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a66652-7f7c-4da6-bb1b-bc029b24bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~((df['Disease'] == 2) & (df['File_Count'] == 0))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f1aae0-1010-43b2-a270-a81881a900ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Disease'] == 2) & (df['File_Count'] == 0)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0df0dc-ae01-4881-be61-654105345554",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Disease'] == 2)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b15b2-de02-4495-92ac-b30d429c00d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "Fluency_files = [f for f in os.listdir(Peripheral_Neuropathy_Fluency) if f.endswith(\".wav\")]\n",
    "\n",
    "def extract_person_code(filename):\n",
    "    match = re.search(r'output_PN_(.+)_\\d+\\.wav', filename)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "person_code_to_files = {}\n",
    "for f in Fluency_files:\n",
    "    code = extract_person_code(f)\n",
    "    if code:\n",
    "        person_code_to_files.setdefault(code, []).append(f)\n",
    "\n",
    "def get_matching_fluency_files(row):\n",
    "    fname = row['Merged_File_ids']\n",
    "    match = re.search(r'ID-\\d{2}-\\d{2}-[A-Z]-(.+?)\\.wav', fname)\n",
    "    if match:\n",
    "        person_code = match.group(1).strip()\n",
    "        return person_code_to_files.get(person_code, [])\n",
    "    return []\n",
    "\n",
    "df['File_ids'] = df.apply(get_matching_fluency_files, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b43bf-7173-426a-9017-ccbd88b62788",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('File_ids').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5959b68a-5af5-4127-b508-9c35e82c94d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['Disease'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c7035-6473-48ba-a733-4e84019f734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Disease'] == 2].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df8358-f112-42d4-a108-c0a5fa6f3de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4296329-9f27-49dc-b457-66758e0d9c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 'Duration', 'Syllable' extraction\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_syllable_rate_for_files(file_list, folder_path):\n",
    "    total_duration = 0\n",
    "    total_syllables = 0\n",
    "\n",
    "    for filename in file_list:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None)\n",
    "            duration = librosa.get_duration(y=y, sr=sr)\n",
    "            onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "            onsets = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr, units='time')\n",
    "            syllable_count = len(onsets)\n",
    "\n",
    "            total_duration += duration\n",
    "            total_syllables += syllable_count\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {filename} - {e}\")\n",
    "            continue\n",
    "\n",
    "    return total_duration, total_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380aa556-c7e7-436d-8c6d-ddf081cf3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_full_syllable_analysis(row):\n",
    "    result = {\n",
    "        'Duration': 0.0,\n",
    "        'Syllable': 0.0,\n",
    "    }\n",
    "\n",
    "    match_files = row['File_ids']\n",
    "\n",
    "    if isinstance(match_files, str):\n",
    "        match_files = [match_files]\n",
    "    elif isinstance(match_files, list):\n",
    "        match_files = match_files\n",
    "    else:\n",
    "        match_files = []\n",
    "    \n",
    "    if match_files:\n",
    "        total_duration, total_syllables = analyze_syllable_rate_for_files(match_files, Peripheral_Neuropathy_Fluency)\n",
    "        result.update({\n",
    "            'Duration': total_duration,\n",
    "            'Syllable': total_syllables,\n",
    "        })\n",
    "        \n",
    "    return pd.Series(result)\n",
    "\n",
    "df_disease2 = df[df['Disease'] == 2].copy()\n",
    "\n",
    "df_disease2[['Duration', 'Syllable']] = df_disease2.apply(apply_full_syllable_analysis, axis=1)\n",
    "\n",
    "df[['Duration', 'Syllable']] = 0.0\n",
    "\n",
    "df.update(df_disease2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea2d88-2525-458f-a83a-76f3c25e0878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Disease'] == 2].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c62631-68dd-4371-b848-720e0d9d04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Speak_Time' extraction\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def estimate_speaking_ratio(file_list, folder_path, vad_threshold_db=-30):\n",
    "    total_duration = 0\n",
    "    total_speaking_time = 0\n",
    "\n",
    "    for filename in file_list:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None)\n",
    "            duration = librosa.get_duration(y=y, sr=sr)\n",
    "            total_duration += duration\n",
    "\n",
    "            intervals = librosa.effects.split(y, top_db=abs(vad_threshold_db))\n",
    "            speaking_segments = sum((end - start) for start, end in intervals)\n",
    "            speaking_time = speaking_segments / sr\n",
    "            total_speaking_time += speaking_time\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {filename} - {e}\")\n",
    "\n",
    "    return total_speaking_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e82dfe-5c4b-4182-8ea0-f344c3c766f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_speaking_ratio(row):\n",
    "    result = {\n",
    "        'Speak_Time': 0.0\n",
    "    }\n",
    "\n",
    "    match_files = row['File_ids']\n",
    "\n",
    "    if isinstance(match_files, str):\n",
    "        match_files = [match_files]\n",
    "    elif isinstance(match_files, list):\n",
    "        match_files = match_files\n",
    "    else:\n",
    "        match_files = []\n",
    "    \n",
    "    if match_files:\n",
    "        total_speaking_time = estimate_speaking_ratio(match_files, Peripheral_Neuropathy_Fluency)\n",
    "        result.update({\n",
    "            'Speak_Time': total_speaking_time\n",
    "        })\n",
    "        \n",
    "    return pd.Series(result)\n",
    "\n",
    "df_disease2 = df[df['Disease'] == 2].copy()\n",
    "\n",
    "df_disease2[['Speak_Time']] = df_disease2.apply(apply_speaking_ratio, axis=1)\n",
    "\n",
    "df[['Speak_Time']] = 0.0\n",
    "\n",
    "df.update(df_disease2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f87e1-7d14-43cd-9ba9-8d0e73447809",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Disease'] == 2].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7733cba8-e061-4b02-9200-a9dbf6a2db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pause_Time, Pause_Count extraction\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def estimate_pause_ratio(file_list, folder_path, pause_threshold_sec=0.5, top_db=30):\n",
    "    total_audio_duration = 0.0\n",
    "    total_pause_time = 0.0\n",
    "    total_pause_count = 0\n",
    "\n",
    "    for filename in file_list:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None)\n",
    "            duration = librosa.get_duration(y=y, sr=sr)\n",
    "            total_audio_duration += duration\n",
    "\n",
    "            intervals = librosa.effects.split(y, top_db=top_db)  # [[start, end], ...]\n",
    "\n",
    "            for i in range(1, len(intervals)):\n",
    "                prev_end = intervals[i-1][1]\n",
    "                curr_start = intervals[i][0]\n",
    "                gap_duration = (curr_start - prev_end) / sr  # sample → sec\n",
    "\n",
    "                if gap_duration >= pause_threshold_sec:\n",
    "                    total_pause_time += gap_duration\n",
    "                    total_pause_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {filename} - {e}\")\n",
    "\n",
    "    return total_pause_time, total_pause_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992f81f-8af4-4f4c-be44-6d9f71e545d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pause_ratio(row):\n",
    "    result = {\n",
    "        'Pause_Time': 0.0,\n",
    "        'Pause_Count': 0.0\n",
    "    }\n",
    "    match_files = row['File_ids']\n",
    "\n",
    "    if isinstance(match_files, str):\n",
    "        match_files = [match_files]\n",
    "    elif isinstance(match_files, list):\n",
    "        match_files = match_files\n",
    "    else:\n",
    "        match_files = []\n",
    "    \n",
    "    if match_files:\n",
    "        total_pause_time, total_pause_count = estimate_pause_ratio(match_files, Peripheral_Neuropathy_Fluency)\n",
    "        result.update({\n",
    "            'Pause_Time': total_pause_time,\n",
    "            'Pause_Count': total_pause_count\n",
    "        })\n",
    "        \n",
    "    return pd.Series(result)\n",
    "\n",
    "df_disease2 = df[df['Disease'] == 2].copy()\n",
    "\n",
    "df_disease2[['Pause_Time', 'Pause_Count']] = df_disease2.apply(apply_pause_ratio, axis=1)\n",
    "\n",
    "df[['Pause_Time', 'Pause_Count']] = 0.0\n",
    "\n",
    "df.update(df_disease2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff80348-6414-4e4a-bec1-ec3e502f781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Disease'] == 2].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75cdd5e-d032-4b3f-b7b7-9bdd077c95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speak_Count extraction\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def estimate_continuity(file_list, folder_path, top_db=30, pause_threshold_sec=0.5):\n",
    "    total_speaking_time = 0.0\n",
    "    total_speech_segments = 0\n",
    "\n",
    "    for filename in file_list:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "            intervals = librosa.effects.split(y, top_db=top_db)\n",
    "\n",
    "            segment_speaking_time = sum((end - start) for start, end in intervals) / sr\n",
    "            total_speaking_time += segment_speaking_time\n",
    "\n",
    "            if len(intervals) == 0:\n",
    "                continue\n",
    "            segment_count = 1\n",
    "\n",
    "            for i in range(1, len(intervals)):\n",
    "                prev_end = intervals[i - 1][1]\n",
    "                curr_start = intervals[i][0]\n",
    "                gap = (curr_start - prev_end) / sr\n",
    "\n",
    "                if gap >= pause_threshold_sec:\n",
    "                    segment_count += 1\n",
    "\n",
    "            total_speech_segments += segment_count\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {filename} - {e}\")\n",
    "\n",
    "    return total_speech_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d010b45-7839-45df-a634-ea88b81623ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_continuity(row):\n",
    "    result = {\n",
    "        'Speak_Count': 0.0\n",
    "    }\n",
    "\n",
    "    match_files = row['File_ids']\n",
    "\n",
    "    if isinstance(match_files, str):\n",
    "        match_files = [match_files]\n",
    "    elif isinstance(match_files, list):\n",
    "        match_files = match_files\n",
    "    else:\n",
    "        match_files = []\n",
    "    \n",
    "    if match_files:\n",
    "        total_speech_segments = estimate_continuity(match_files, Peripheral_Neuropathy_Fluency)\n",
    "        result.update({\n",
    "            'Speak_Count': total_speech_segments,\n",
    "        })\n",
    "        \n",
    "    return pd.Series(result)\n",
    "\n",
    "df_disease2 = df[df['Disease'] == 2].copy()\n",
    "\n",
    "df_disease2[['Speak_Count']] = df_disease2.apply(apply_continuity, axis=1)\n",
    "\n",
    "df[['Speak_Count']] = 0.0\n",
    "\n",
    "df.update(df_disease2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db24fa-0ce0-4378-805f-c272c845e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Disease'] == 2].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95296b-c25c-4389-b62e-71fe255f24d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa praat-parselmouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3decc-7b5f-4911-b6c9-efa0026dc72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def safe_get_mean(obj, method, *args):\n",
    "    try:\n",
    "        val = call(obj, method, *args)\n",
    "        return 0.0 if np.isnan(val) else val\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def extract_core_acoustic_features(file_path):\n",
    "    snd = parselmouth.Sound(file_path)\n",
    "    # Pitch\n",
    "    pitch = snd.to_pitch(pitch_floor=50, pitch_ceiling=600)\n",
    "    pitch_mean = safe_get_mean(pitch, \"Get mean\", 0, 0, \"Hertz\")\n",
    "    # Jitter\n",
    "    pp = call(snd, \"To PointProcess (periodic, cc)\", 75, 500)\n",
    "    jitter = safe_get_mean(pp, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    # Shimmer\n",
    "    shimmer = safe_get_mean([snd, pp], \"Get shimmer (local)\",\n",
    "                            0,0,0.0001,0.02,1.3,1.6)\n",
    "    # HNR\n",
    "    hnr = safe_get_mean(snd.to_harmonicity_cc(), \"Get mean\", 0, 0)\n",
    "    # Formants\n",
    "    formant = snd.to_formant_burg(time_step=0.01,\n",
    "                                  max_number_of_formants=5,\n",
    "                                  maximum_formant=5500,\n",
    "                                  window_length=0.025,\n",
    "                                  pre_emphasis_from=50)\n",
    "    f1 = safe_get_mean(formant, \"Get mean\", 1, 0, 0, \"Hertz\")\n",
    "    f2 = safe_get_mean(formant, \"Get mean\", 2, 0, 0, \"Hertz\")\n",
    "    f3 = safe_get_mean(formant, \"Get mean\", 3, 0, 0, \"Hertz\")\n",
    "    # MFCC\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfcc_means = np.mean(mfccs, axis=1)\n",
    "\n",
    "    features = {\n",
    "        'Pitch_Mean': pitch_mean,\n",
    "        'Jitter_Local': jitter,\n",
    "        'Shimmer_Local': shimmer,\n",
    "        'HNR': hnr,\n",
    "        'formant1_mean': f1,\n",
    "        'formant2_mean': f2,\n",
    "        'formant3_mean': f3,\n",
    "        **{f'mfcc_{i+1}': mfcc_means[i] for i in range(13)}\n",
    "    }\n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636caa00-b887-4bd0-a7e0-66f89ac8be4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'Pitch_Mean',\n",
    "    'Jitter_Local',\n",
    "    'Shimmer_Local',\n",
    "    'HNR',\n",
    "    'formant1_mean',\n",
    "    'formant2_mean',\n",
    "    'formant3_mean',\n",
    "] + [f'mfcc_{i}' for i in range(1, 14)]\n",
    "\n",
    "for col in feature_columns:\n",
    "    df[col] = 0.0\n",
    "\n",
    "def apply_acoustic_features_by_file(row):\n",
    "    file_name = row['File_ids']\n",
    "    if isinstance(file_name, str):\n",
    "        fpath = os.path.join(Peripheral_Neuropathy_Fluency, file_name)\n",
    "        if os.path.isfile(fpath):\n",
    "            return extract_core_acoustic_features(fpath)\n",
    "    return pd.Series({col: 0.0 for col in feature_columns})\n",
    "\n",
    "mask = df['Disease'] == 2\n",
    "df.loc[mask, feature_columns] = df.loc[mask].apply(\n",
    "    apply_acoustic_features_by_file, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21e364-c8c1-4223-b999-34f1656f4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Disease'] == 2].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c6e79-9d4e-4148-8089-e5e8e3c55c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "mfcc_cols = [f\"mfcc_{i}\" for i in range(1, 14)]\n",
    "formant_cols = [\"formant1_mean\", \"formant2_mean\", \"formant3_mean\"]\n",
    "\n",
    "mfcc_pca = PCA(n_components=1)\n",
    "formant_pca = PCA(n_components=1)\n",
    "\n",
    "df[\"MFCC\"] = mfcc_pca.fit_transform(df[mfcc_cols]).ravel()\n",
    "df[\"Formant\"] = formant_pca.fit_transform(df[formant_cols]).ravel()\n",
    "\n",
    "print(\"MFCC PC1 explained variance ratio:\", mfcc_pca.explained_variance_ratio_[0])\n",
    "print(\"Formant PC1 explained variance ratio:\", formant_pca.explained_variance_ratio_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037c8a5-48ba-4b13-a415-fd5575aa02c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('formant1_mean').reset_index(drop=True)\n",
    "df = df.explode('formant2_mean').reset_index(drop=True)\n",
    "df = df.explode('formant3_mean').reset_index(drop=True)\n",
    "\n",
    "df = df.explode('mfcc_1').reset_index(drop=True)\n",
    "df = df.explode('mfcc_2').reset_index(drop=True)\n",
    "df = df.explode('mfcc_3').reset_index(drop=True)\n",
    "df = df.explode('mfcc_4').reset_index(drop=True)\n",
    "df = df.explode('mfcc_5').reset_index(drop=True)\n",
    "df = df.explode('mfcc_6').reset_index(drop=True)\n",
    "df = df.explode('mfcc_7').reset_index(drop=True)\n",
    "df = df.explode('mfcc_8').reset_index(drop=True)\n",
    "df = df.explode('mfcc_9').reset_index(drop=True)\n",
    "df = df.explode('mfcc_10').reset_index(drop=True)\n",
    "df = df.explode('mfcc_11').reset_index(drop=True)\n",
    "df = df.explode('mfcc_12').reset_index(drop=True)\n",
    "df = df.explode('mfcc_13').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('Initial').reset_index(drop=True)\n",
    "df = df.explode('Area').reset_index(drop=True)\n",
    "df = df.explode('Merged_File_ids').reset_index(drop=True)\n",
    "df = df.explode('File_Count').reset_index(drop=True)\n",
    "df = df.explode('Duration').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c16e2-67cd-4328-8cc4-2b84ad2c6efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Disease'] == 2].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3a5e9-95f8-4899-b194-47a6556cd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Peripheral_Neuropathy_feature.csv\", index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
